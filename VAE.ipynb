{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMY3DGuEC0zQZi7O1rUc6/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbrandl/nn_drsynth/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCiRSnnLFVD9"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "*   **[Torch](https://pytorch.org/docs/)** is a neural network library.\n",
        "*   **[IPython](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html)** is used for displaying and playing audio samples in this notebook.\n",
        "*   **google.colab** is used to get access to Google Drive for importing the dataset file.\n",
        "*   **pickle** is used to convert the dataset file to its original object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgXnvfOHvkYx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from IPython.display import Audio\n",
        "from IPython.core.display import display\n",
        "\n",
        "# For Type Annotations\n",
        "from typing import Generator, Mapping, Tuple, NamedTuple, Sequence, Callable\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LcWwcWQ4AKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cecb96e-4342-46d1-8f11-5a8129e930cf"
      },
      "source": [
        "# @title Set Working Directories\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive') \n",
        "project_path = '/content/drive/My Drive/nn_drum' # @param\n",
        "dataset_path = '/content/drive/My Drive/nn_drum/snares_tensor.db' # @param\n",
        "\n",
        "# Select the Processing Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Working on {}.\".format(device))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Working on cpu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA9_KpkGwO7m"
      },
      "source": [
        "# @title Load the Dataset\n",
        "# @markdown The dataset is loaded as a numpy array, then shaped accoriding to the size of the batch\n",
        "batch_size =  5 # @param\n",
        "# @markdown The sample rate of the samples in the database\n",
        "sample_rate = 44100 # @param\n",
        "\n",
        "def load_dataset(path, to_numpy=False):\n",
        "    with open(path, 'rb') as file:\n",
        "        tensor = pickle.load(file)\n",
        "        return tensor.numpy() if to_numpy else tensor\n",
        "\n",
        "dataset = load_dataset(dataset_path, to_numpy=True)\n",
        "num_data = dataset.shape[0]\n",
        "dataset = dataset.reshape(num_data // batch_size, batch_size, -1)\n",
        "print(f\"Dataset shape {dataset.shape} with (num_batches, batch_size, sample_length).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QvGaQmY1SIe"
      },
      "source": [
        "#@markdown Listen to a sample\n",
        "Audio(dataset[0, 0, :], rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQULnxUMdHxy"
      },
      "source": [
        "# Variational Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzYpQDSV84vM"
      },
      "source": [
        "#@title VAE classes and Train Funcitions\n",
        "#@markdown VAEs sample from a Gaussian distribution. To optimize this distribution the ELBO $\\mathcal{L}$ is maximized:  \n",
        "\n",
        "#@markdown $$ \\mathcal{L}(x) = \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\big[\\log p_\\theta(x | z)\\big] - \\mathbb{KL}\\big(q_\\phi(z | x) || p(z)\\big).$$\n",
        "\n",
        "#@markdown * $q_\\phi(z|x)$ is the Gaussian posterior; the inferred latent distribution corresponding to the input $x$,\n",
        "#@markdown * $p_\\theta(x|z)$ is the likelihood of $x$ for latent $z$ according to our model,\n",
        "#@markdown * $p(z)$ is our chosen prior, $\\mathcal{N}(0, I)$, the centered unit variance gaussian. \\\\\n",
        "\n",
        "#@markdown The KL divergence can be derived analytically as follow:\n",
        "#@markdown $$ \\mathbb{KL}\\big( \\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1) \\big) = \\frac12 \\big(\\sigma^2  - \\log(\\sigma^2) + \\mu^2 - 1 \\big).$$\n",
        "\n",
        "\n",
        "class AdaptiveActivation(nn.Module):\n",
        "    \"\"\"\n",
        "        Adaptive activation function according to Em Karniadakis 2020.\n",
        "        Title: \"On the convergence of physics informed neural networks for linear \n",
        "        second-order elliptic and parabolic type PDEs\"\n",
        "    \"\"\"\n",
        "    def __init__(self, activation_fun: Callable):\n",
        "        super(AdaptiveActivation, self).__init__()\n",
        "        self.n = torch.Tensor([10]).to(device)\n",
        "        self.a = nn.Parameter(torch.rand(1))\n",
        "        self.activation_fun = activation_fun()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.activation_fun(self.n * self.a * x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent: int):\n",
        "        super().__init__()\n",
        "        self.latent = latent\n",
        "        self.enc_l1 = nn.Linear(in_features=2**14, out_features=2**12)\n",
        "        self.adapt_leakyReLU = AdaptiveActivation(nn.LeakyReLU)\n",
        "        self.enc_l2 = nn.Linear(in_features=2**12, out_features=latent*2)\n",
        "        self.adapt_tanh = AdaptiveActivation(nn.Tanh)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x = self.adapt_leakyReLU(self.enc_l1(x))\n",
        "        x = self.adapt_tanh(self.enc_l2(x))\n",
        "        mu, logvar = x[:self.latent], x[self.latent:]\n",
        "        return mu, logvar\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent):\n",
        "        super().__init__()\n",
        "        self.dec_l1 = nn.Linear(in_features=latent, out_features=2**12)\n",
        "        self.adapt_leakyReLU = AdaptiveActivation(nn.LeakyReLU)\n",
        "        self.dec_l2 = nn.Linear(in_features=2**12, out_features=2**14)\n",
        "        self.adapt_tanh = AdaptiveActivation(nn.Tanh)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        x = self.adapt_leakyReLU(self.dec_l1(x))\n",
        "        x = self.adapt_tanh(self.dec_l2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(latent)\n",
        "        self.decoder = Decoder(latent)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, mean, logvar\n",
        "\n",
        "    def generate(self, key):\n",
        "        rng, key = jax.random.split(key)\n",
        "        sampled_z = jax.random.normal(key, self.latents)\n",
        "        return self.decoder(sampled_z)\n",
        "\n",
        "    def reparameterize(self, mu: Tensor, log_var: Tensor):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5*log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            sample = mu + (eps * std)\n",
        "            return sample\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "\n",
        "def vae_loss(x_recon: Tensor, x: Tensor, mu: Tensor, logvar: Tensor):\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return F.mse_loss(x_recon, x) - kl_div\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}